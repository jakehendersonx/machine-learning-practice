{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoder\n",
    "\n",
    "Variational autoencoders are often associated with the autoencoder model because of its architectural affinity, but with significant differences in the goal and mathematical formulation. Variational autoencoders are probabilistic generative models that require neural networks as only a part of their overall structure. The neural network components are typically referred to as the encoder and decoder for the first and second component respectively. The first neural network maps the input variable to a latent space that corresponds to the parameters of a variational distribution. In this way, the encoder can produce multiple different samples that all come from the same distribution. The decoder has the opposite function, which is to map from the latent space to the input space, in order to produce or generate data points. Both networks are typically trained together with the usage of the reparameterization trick, although the variance of the noise model can be learned separately.\n",
    "\n",
    "Although this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning.\n",
    "\n",
    "encoder: input Layer -> hidden layer -> output layer\n",
    "decoder: (previous output layer) -> hidden later -> resconstructed input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(\n",
    "    nn.Linear(28*28, 128), # input layer\n",
    "    nn.ReLU(),             # activation function\n",
    "    nn.Linear(128, 64)     # output layer\n",
    ")\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(28*28, 128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0083,  0.2178, -0.2849,  ..., -0.4853,  1.3087,  0.0836],\n",
      "        [-0.1319,  0.1294, -0.9456,  ..., -0.2777,  0.0894,  0.2237],\n",
      "        [-0.8708,  0.1305,  1.4107,  ..., -0.5334, -1.8840, -1.4724],\n",
      "        ...,\n",
      "        [ 0.8371,  0.7285, -0.6264,  ...,  1.0658, -0.5958, -0.3265],\n",
      "        [-0.6174,  0.8456,  1.9640,  ...,  1.5360,  0.6284, -0.4803],\n",
      "        [-1.3341,  1.0296,  0.2186,  ..., -0.4773, -1.2619,  1.0759]])\n",
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "inpt = torch.randn(128, 20)\n",
    "print(inpt)\n",
    "out = m(inpt)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
