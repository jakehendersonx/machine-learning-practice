{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakehenderson/Documents/code/projects/machine-learning-practice/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 2.36Git/s]                                                     \n",
      "Fetching encoder.json: 1.05Mit [00:00, 3.05Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 9.20Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:30, 16.1Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 5.70Git/s]                                               \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.69Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 4.23Mit/s]                                                      \n",
      "/Users/jakehenderson/Documents/code/projects/machine-learning-practice/.venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model (download if not already downloaded)\n",
    "gpt2.download_gpt2()\n",
    "\n",
    "# Load pre-trained SentenceTransformer model\n",
    "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample page of notes (replace this with your actual text)\n",
    "notes = \"\"\"\n",
    "#bubble \n",
    "When I write, or use the computer, I want to see some stuff that I have written before. \n",
    "I want there to be a recommendation alorithm that points out to me stuff that I have jotted down. \n",
    "Quotes, ideas, plans, technical notes. I want to be reminded of when and what I was thinking in the past. \n",
    "\n",
    "How am I using reminders?\n",
    "today, this week, longterm, then specific projects\n",
    "\n",
    "how am I using notes:\n",
    "thats really all kinds of things\n",
    "I have used notion\n",
    "I have used obsidian\n",
    "\n",
    "seems like what I want to be able to do is give a command\n",
    "about what I am writing and be writing in that vein the entire time.\n",
    "\n",
    "if I say note: I am writing a multiline note.\n",
    "if I say bubble: I am writing a minimal quick idea.\n",
    "if I say quote I am writing a quote.\n",
    "\n",
    "honestly we need a deepwork station that works for us\n",
    "I don’t want to have to look up or risk breaking focus \n",
    "\n",
    "we have to be able to setup commands to do certain other tasks\n",
    "we have to build an ontology of relatedness\n",
    "\n",
    "want to be able to say what note or topic\n",
    "\n",
    "If I want to add to the visions I have I want to be able to say\n",
    "visions: whatever I am thingking of at the time.\n",
    "\n",
    "Ocean should have tides\n",
    "tides are subdivisions\n",
    "qoutes would be a tide\n",
    "lets make it just that simple\n",
    "\n",
    "if I go \n",
    "quote:: whatever - Jean-Paul Sartre\n",
    "hopefully in the UI we can color how each Tide looks \n",
    "tides can have a subtide, the tide should really be the unique way that we think of “what we are talking about”\n",
    " IF the user has to do too much work to make like things like, its really not helpful. Its obsidian and Notion all over again with the working\n",
    "\n",
    "It has too feel like this stuff all flies from the users fingertips\n",
    "\n",
    "basically a tide is a topic, and the bubble is the content\n",
    "there should be special support for certain types of topic\n",
    "a quote for example should present as a quote and a subject\n",
    "\n",
    "writing quote::: should then show the rest that way as you go\n",
    "“italic whaever the person said” - who they were\n",
    "\n",
    "bubbles are rust code\n",
    "what are topics, where do the topics go\n",
    "\n",
    "its a graph where the edges have names isn’t it\n",
    "But what is the program\n",
    "I think there should be \n",
    "Ocean:\n",
    "\ttopics: Vec<String>\n",
    "\tbubbles: <Vec> Bubble\n",
    "\twhat else does the ocean need to do other than convey the state\n",
    "\tthe ocean is essentially a cache for which the state of the application is maintained\n",
    "\tit cannot be duplicated and there can be only one instance at a time\n",
    "\t\n",
    "lets give some thanks here. Here we are using the greatest language ever written to write an app I have\n",
    "been thinking about building for years now. Life is good. you never know how it will go. Let your anxieties melt\n",
    "away and \n",
    "\n",
    "removing a bubble is an interesting thing. Is there a way we can remove it once and have it be done? What if a bubble has not topic?\n",
    "is it instantly ocean?\n",
    "\n",
    "you can make a bubble without a topic\n",
    "you can make a topic without a bubble\n",
    "I think its just not a good idea to have \n",
    "the topic of a given bubble saved inside the bubble\n",
    "\n",
    "Removing is really secondary, Its kinda not the point of this stuff\n",
    "ther persistance part of all of this is a waste of time for a demo anyway\n",
    "\n",
    "Its really about that UI portion\n",
    "then do the lookup bullshit\n",
    "then do the \n",
    "\n",
    "Its worth thinking about the bubble to topic relationship think as a graph\n",
    "Its an adjacentcy list\n",
    "\n",
    "need a decent text box with a send button, need to know what the fuck we put in there in the first place\n",
    "will actually be nice to use when we get that shit going for the first time, and I can finally stare into the abyss and think\n",
    "\n",
    "The app should be a rest API that has one endpoint and goes from there.\n",
    "I guess it will also need get ocean and such\n",
    "\n",
    "would even be nice to see this used for taking notes about this very app tbh\n",
    "\n",
    "can topics have topics?\n",
    "notes\n",
    "\n",
    "API\n",
    "OCEAN (app state)\n",
    "db -> JSON serde at this point\n",
    "\n",
    "Talk out the flow\n",
    "User logs in, sees all their bubbles -> getAllBubbles\n",
    "\n",
    "API:\n",
    "get all\n",
    "write new\n",
    "parse new\n",
    "do command\n",
    "\n",
    "UI should be all about displaying this stuff well\n",
    "finally data manipulation as you go\n",
    "\n",
    "There has to be a command interpreter and thus a parser of what the user has written \n",
    "Would be really cool if I could get NLP down to let them basically use any basic way to ask for stuff\n",
    "\n",
    "Have to get voice down. Has to be a quick button click to get a note and topic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m cluster_sentences \u001b[38;5;241m=\u001b[39m [sentences[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cluster_model\u001b[38;5;241m.\u001b[39mlabels_)) \u001b[38;5;28;01mif\u001b[39;00m cluster_model\u001b[38;5;241m.\u001b[39mlabels_[i] \u001b[38;5;241m==\u001b[39m cluster_id]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Generate a brief subject description using GPT-2\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m subject_description \u001b[38;5;241m=\u001b[39m gpt2\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43msess\u001b[49m, prefix\u001b[38;5;241m=\u001b[39mcluster_sentences[\u001b[38;5;241m0\u001b[39m], length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, return_as_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Save to a text file\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_subject_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_description[:\u001b[38;5;241m20\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize the text into sentences using spaCy\n",
    "sentences = [sent.text for sent in nlp(notes).sents]\n",
    "\n",
    "# Generate GPT-2 embeddings for each sentence\n",
    "def get_sentence_embeddings(sentences):\n",
    "    return bert_model.encode(sentences)\n",
    "\n",
    "# Hierarchical clustering\n",
    "sentence_embeddings = get_sentence_embeddings(sentences)\n",
    "cluster_model = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage=\"ward\").fit(sentence_embeddings)\n",
    "num_clusters = len(np.unique(cluster_model.labels_))\n",
    "\n",
    "# Create text files for each cluster with improved subject descriptions\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_sentences = [sentences[i] for i in range(len(cluster_model.labels_)) if cluster_model.labels_[i] == cluster_id]\n",
    "    \n",
    "    # Generate a brief subject description using GPT-2\n",
    "    subject_description = gpt2.generate(sess, prefix=cluster_sentences[0], length=50, temperature=0.7, return_as_list=True)[0]\n",
    "    \n",
    "    # Save to a text file\n",
    "    with open(f\"cluster_{cluster_id}_subject_{subject_description[:20]}.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(cluster_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 61, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m cluster_sentences \u001b[38;5;241m=\u001b[39m [sentences[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cluster_model\u001b[38;5;241m.\u001b[39mlabels_)) \u001b[38;5;28;01mif\u001b[39;00m cluster_model\u001b[38;5;241m.\u001b[39mlabels_[i] \u001b[38;5;241m==\u001b[39m cluster_id]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Generate a brief subject description using GPT-2\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m subject_description \u001b[38;5;241m=\u001b[39m gpt2_tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mgpt2_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt2_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Save to a text file\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_subject_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_description[:\u001b[38;5;241m20\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/Documents/code/projects/machine-learning-practice/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/projects/machine-learning-practice/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1466\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1461\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `generation_config` defines a `cache_implementation` that is not compatible with this model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1462\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure it has a `_setup_cache` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1463\u001b[0m         )\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_cache(cache_cls, max_batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_cache_len\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_generation_mode(generation_config, assistant_model)\n",
      "File \u001b[0;32m~/Documents/code/projects/machine-learning-practice/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1186\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1185\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     )\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1196\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 61, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load pre-trained SentenceTransformer model\n",
    "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Sample page of notes (replace this with your actual text)\n",
    "\n",
    "# Tokenize the text into sentences using spaCy\n",
    "sentences = [sent.text for sent in nlp(notes).sents]\n",
    "\n",
    "# Generate GPT-2 embeddings for each sentence\n",
    "def get_sentence_embeddings(sentences):\n",
    "    return bert_model.encode(sentences)\n",
    "\n",
    "# Hierarchical clustering\n",
    "sentence_embeddings = get_sentence_embeddings(sentences)\n",
    "cluster_model = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage=\"ward\").fit(sentence_embeddings)\n",
    "num_clusters = len(np.unique(cluster_model.labels_))\n",
    "\n",
    "# Create text files for each cluster with improved subject descriptions\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_sentences = [sentences[i] for i in range(len(cluster_model.labels_)) if cluster_model.labels_[i] == cluster_id]\n",
    "    \n",
    "    # Generate a brief subject description using GPT-2\n",
    "    subject_description = gpt2_tokenizer.decode(gpt2_model.generate(gpt2_tokenizer.encode(cluster_sentences[0], return_tensors=\"pt\"), max_length=50)[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Save to a text file\n",
    "    with open(f\"cluster_{cluster_id}_subject_{subject_description[:20]}.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(cluster_sentences))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
